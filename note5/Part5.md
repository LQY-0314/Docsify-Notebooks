# 第五章&ensp;生成学习算法

学习至此，我们对于机器学习的学习过程有了一个大致的了解。我们之前所学习到的算法，总是对给定 $ x $ 时的 $ y $ 的条件分布进行建模。例如，逻辑回归中 $ p(y|x; \theta) $ 作为 $ h_\theta(x) = g(\theta^T x) $，其中 $ g $ 是 sigmoid 函数。

试图直接学习 $ p(y|x) $（例如逻辑回归）的算法，或者试图从输入空间 $ \mathcal{X} $ 直接学习映射到标签 $\{0, 1\}$ 的算法（如感知机）被称为 **判别学习算法（discriminative learning algorithms）**。而在刚才我们所举的例子中，是对 $ p(x|y) $（和$ p(y) $）进行建模的算法。这种算法被称为 **生成学习算法（generative learning algorithms）**。

**判别学习算法试图直接学习 $ p(y|x) $，而生成学习算法试图学习 $ p(x|y) $ 和 $ p(y) $。**

通过下面的学习，应该对以下知识有着基本的了解：

* 多元正态分布
* 拉普拉斯平滑

通过下面的学习，应该重点掌握：

* 什么是判别学习算法
* 什么是生成学习算法
* 高斯判别分析 GDA
* 朴素贝叶斯

- - -

### 高斯判别分析

我们首先将要接触的生成学习算法是 **高斯判别分析（Gaussian discriminant analysis）**。

在此之前，我们需要进行一些数学上的准备。

#### 数学准备

一般地，在之前的算法中，我们试图得到 $ p(y|x) $。已知自变量，来预测因变量，这种概率被称为 **后验概率（class posterioris）**。在生成学习算法中，已知因变量，来预测自变量，这种概率被称为 **先验概率（class prioris）**。

> **<font size = 4>先验与后验</font>**<br>
> **简单地讲，先验一般指的是经验性（旧信息），而后验指的是考虑新信息。**

通过贝叶斯公式求得 $ p(y|x) $：

<div class="math">
$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$
</div>

> **<font size = 4>贝叶斯公式</font>**<br>
> **贝叶斯公式是贝叶斯学派的核心公式，它将先验概率和后验概率联系起来。**<br>
> **设试验 $ E $的样本空间为 $ S $， $ A $ 为 $ E $ 的事件，$ B_1,B_2,...,B_n $ 为 $ S $ 的一个划分，且 $ p(A) > 0,p(B_i) > 0 $，有：**<br>
>**$$
>p(B_i|A) = \frac{p(A|B_i)p(B_i)}{p(A)}
>$$**
>**其中 $ p(A) = \sum\limits_{j=1}^{n}p(A|B_j)p(B_j)$，由全概率公式：$ p(A) = p(A|B_1)p(B_1) + ... + p(A|B_n)p(B_n) $ 导出**

由于我们关心的是 $ y $ 离散结果中哪一个的概率更大，而不是要求得具体的概率，所以上面的公式我们可以表达为：

<div class="math">
$$
\arg \max_y p(y|x) = \arg \max_y \frac{p(x|y)p(y)}{p(x)} = \arg \max_y p(x|y)p(y)
$$
</div>

$ p(x) = 1 $ 表示 $ p(x) $ 涵盖了所有的情况。

#### * 多元正态分布

多元正态分布

#### GDA 模型

#### Extra: GDA 与 逻辑回归